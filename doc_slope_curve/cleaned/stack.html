<p> I have a computer vision algorithm I want to tune up using scipy.optimize.minimize . Right now I only want to tune up two parameters but the number of parameters might eventually grow so I would like to use a technique that can do high-dimensional gradient searches. The Nelder-Mead implementation in SciPy seemed like a good fit. </p> <p> I got the code all set up but it seems that the minimize function really wants to use floating point values with a step size that is less than one.The current set of parameters are both integers and one has a step size of one and the other has a step size of two (i.e. the value must be odd, if it isn't the thing I am trying to optimize will convert it to an odd number). Roughly one parameter is a window size in pixels and the other parameter is a threshold (a value from 0-255). </p> <p> For what it is worth I am using a fresh build of scipy from the git repo. Does anyone know how to tell scipy to use a specific step size for each parameter? Is there some way I can roll my own gradient function? Is there a scipy flag that could help me out? I am aware that this could be done with a simple parameter sweep, but I would eventually like to apply this code to much larger sets of parameters. </p> <p> The code itself is dead simple: </p> <pre> <code> import numpy as np from scipy . optimize import minimize from ScannerUtil import straightenImg import bson def doSingleIteration ( parameters ): # do some machine vision magic # return the difference between my value and the truth value parameters = np . array ([ 11 , 10 ]) res = minimize ( doSingleIteration , parameters , method = 'Nelder-Mead' , options ={ 'xtol' : 1e-2 , 'disp' : True , 'ftol' : 1.0 ,}) #not sure if these params do anything print "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~" print res </code> </pre> <p> This is what my output looks like. As you can see we are repeating a lot of runs and not getting anywhere in the minimization. </p> <pre> <code> *+++++++++++++++++++++++++++++++++++++++++ [ 11. 10. ] <-- Output from scipy minimize { 'block_size' : 11 , 'degree' : 10 } <-- input to my algorithm rounded and made int +++++++++++++++++++++++++++++++++++++++++ 120 <-- output of the function I am trying to minimize +++++++++++++++++++++++++++++++++++++++++ [ 11.55 10. ] { 'block_size' : 11 , 'degree' : 10 } +++++++++++++++++++++++++++++++++++++++++ 120 +++++++++++++++++++++++++++++++++++++++++ [ 11. 10.5 ] { 'block_size' : 11 , 'degree' : 10 } +++++++++++++++++++++++++++++++++++++++++ 120 +++++++++++++++++++++++++++++++++++++++++ [ 11.55 9.5 ] { 'block_size' : 11 , 'degree' : 9 } +++++++++++++++++++++++++++++++++++++++++ 120 +++++++++++++++++++++++++++++++++++++++++ [ 11.1375 10.25 ] { 'block_size' : 11 , 'degree' : 10 } +++++++++++++++++++++++++++++++++++++++++ 120 +++++++++++++++++++++++++++++++++++++++++ [ 11.275 10. ] { 'block_size' : 11 , 'degree' : 10 } +++++++++++++++++++++++++++++++++++++++++ 120 +++++++++++++++++++++++++++++++++++++++++ [ 11. 10.25 ] { 'block_size' : 11 , 'degree' : 10 } +++++++++++++++++++++++++++++++++++++++++ 120 +++++++++++++++++++++++++++++++++++++++++ [ 11.275 9.75 ] { 'block_size' : 11 , 'degree' : 9 } +++++++++++++++++++++++++++++++++++++++++ 120 +++++++++++++++++++++++++++++++++++++++++ ~~~ SNIP ~~~ +++++++++++++++++++++++++++++++++++++++++ [ 11. 10.0078125 ] { 'block_size' : 11 , 'degree' : 10 } +++++++++++++++++++++++++++++++++++++++++ 120 Optimization terminated successfully . Current function value : 120.000000 Iterations : 7 Function evaluations : 27 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ status : 0 nfev : 27 success : True fun : 120.0 x : array ([ 11. , 10. ]) message : 'Optimization terminated successfully.' nit : 7 * </code> </pre> </div> <div> <div> python optimization numpy machine-learning scipy </div> </div> <div> <div> <div> <div> share | improve this question </div> </div> <div> <div> <div> asked Aug 29 '12 at 14:56 </div> <div> <div> <img> </img> </div> </div> <div> kscottz <div> 587 2 8 17 </div> </div> </div> </div> </div> </div> </div> <div> <div> <ul> <li> <div> <div> 2 </div> </div> <div> <div> According to the docs, SciPy's Nelder-Mead method uses the Simplex linear programming algorithm. It relies on using non-integral points/step sizes to optimize the function. I'm not familiar with SciPy in general, so there may be a configuration option to get it do what you want. You might also want to look into integer programming ( en.wikipedia.org/wiki/Integer_programming ) as that sounds like what you're trying to accomplish. – Eric G Aug 29 '12 at 15:23 </div> </div> </li> <li> <div> <div> </div> </div> <div> <div> @EricG actually I think thats just a name mixup, the Nelder-Mead "Simplex" works with the geometrical structure of a Simplex. It has nothing to do with the Simplex algorithm from linear programming, and this is nonlinear optimization anyways. – seberg Aug 29 '12 at 17:15 </div> </div> </li> <li> <div> <div> 1 </div> </div> <div> <div> Because of issues like this, parameter tuning for ML algorithms is typically done just with a grid search (often on a logarithmic grid, but for your parameters that doesn't seem necessary). You might do a coarse grid search to find a good region first and then a finer-grained grid search in said region. – Dougal Aug 29 '12 at 17:27 </div> </div> </li> </ul> </div> <div> add a comment | </div> </div> </div> </div> <div> </div> <div> <div> <div> 4 Answers 4 <div> <div> active oldest votes </div> </div> </div> </div> <div> <div> <div> <div> <input> </input> up vote 5 down vote accepted </div> </div> <div> <div> <p> Assuming that the function to minimize is arbitrarily complex (nonlinear), this is a very hard problem in general. It cannot be guaranteed to be solved optimal unless you try every possible option. I do <em> not </em> know if there are any integer constrained nonlinear optimizer (somewhat doubt it) and I will assume you know that Nelder-Mead should work fine if it was a contiguous function. </p> <p> Edit: Considering the comment from @Dougal I will just add here: Set up a coarse+fine grid search first, if you then feel like trying if your Nelder-Mead works (and converges faster), the points below may help... </p> <p> But maybe some points that help: </p> <ol> <li> Considering how the whole integer constraint is very difficult, maybe it would be an option to do some simple interpolation to help the optimizer. It should still converge to an integer solution. Of course this requires to calculate extra points, but it might solve many other problems. (even in linear integer programming its common to solve the unconstrained system first AFAIK) </li> <li> Nelder-Mead starts with N+1 points, these are hard wired in scipy (at least older versions) to <code> (1+0.05) * x0[j] </code> (for <code> j </code> in all dimensions, unless <code> x0[j] </code> is 0), which you will see in your first evaluation steps. Maybe these can be supplied in newer versions, otherwise you could just change/copy the scipy code (it is pure python) and set it to something more reasonable. Or if you feel that is simpler, scale all input variables down so that (1+0.05)*x0 is of sensible size. </li> <li> Maybe you should cache all function evaluations, since if you use Nelder-Mead I would guess you can always run into duplicat evaluation (at least at the end). </li> <li> You have to check how likely Nelder-Mead will just shrink to a single value and give up, because it always finds the same result. </li> <li> You generally must check if your function is well behaved at all... This optimization is doomed if the function does not change smooth over the parameter space, and even then it can easily run into local minima if you should have of those. (since you cached all evaluations - see 2. - you could at least plot those and have a look at the error landscape without needing to do any extra evluations) </li> </ol> </div> <div> <div> <div> share | improve this answer </div> </div> <div> <div> <div> edited Aug 29 '12 at 17:40 </div> <div> </div> <div> <div> </div> </div> </div> </div> <div> <div> <div> answered Aug 29 '12 at 17:13 </div> <div> <div> <img> </img> </div> </div> <div> seberg <div> 6,349 1 21 26 </div> </div> </div> </div> </div> </div> <div> <div> <ul> </ul> </div> <div> add a comment | </div> </div> </div> </div> <div> </div> <div> <div> <div> <div> <input> </input> up vote 2 down vote </div> </div> <div> <div> <p> Unfortunately, Scipy's built-in optimization tools don't easily allow for this. But never fear; it sounds like you have a convex problem, and so you should be able to find a unique optimum, even if it won't be mathematically pretty. </p> <p> Two options that I've implemented for different problems are creating a custom gradient descent algorithm, and using bisection on a series of univariate problems. If you're doing cross-validation in your tuning, your loss function unfortunately won't be smooth (because of noise from cross-validation on different datasets), but will be generally convex. </p> <p> To implement gradient descent numerically (without having an analytical method for evaluating the gradient), choose a test point and a second point that is <code> delta </code> away from your test point in all dimensions. Evaluating your loss function at these two points can allow you to numerically compute a local subgradient. It is important that <code> delta </code> be large enough that it steps outside of local minima created by cross-validation noise. </p> <p> A slower but potentially more robust alternative is to implement bisection for each parameter you're testing. If you know that the problem in jointly convex in your two parameters (or <em> n </em> parameters), you can separate this into <em> n </em> univariate optimization problems, and write a bisection algorithm which recursively hones in on the optimal parameters. This can help handle some types of quasiconvexity (e.g. if your loss function takes a background noise value for part of its domain, and is convex in another region), but requires a good guess as to the bounds for the initial iteration. </p> <p> If you simply snap the requested <code> x </code> values to an integer grid without fixing <code> xtol </code> to map to that gridsize, you risk having the solver request two points within a grid cell, receiving the same output value, and concluding that it is at a minimum.